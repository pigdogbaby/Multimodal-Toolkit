{
  "architectures": [
    "PtForMaskedLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "model_type": "pt",
  "dim_z": 32,
  "dim_g": 128,
  "num_iterations": 4,
  "num_channels": 4,
  "ternary_rank": 8,
  "potential_func_z": "square",
  "potential_func_g": "abs",
  "max_position_embeddings": 1024,
  "initializer_range": 0.02,
  "squared_softmax_eps": 1e-6,
  "tie_word_embeddings": false,
  "rope_theta": 10000.0,
  "rope_scaling": null,
  "dropout_prob_z": 0.1,
  "dropout_prob_h": 0.1,
  "classifier_dropout": null,
  "regularize_z": 1,
  "regularize_h": 0.03125,
  "hidden_size": 32,
  "hidden_act": "silu",
  "layer_norm_eps": 1e-05,
  "output_heads": false,
  "output_qzs": false,
  "vocab_size": 32000,
  "hard": true,
  "cpd": true
}